Okay, so in this video I want to go through Lang Extract, a new library that's come out of Google to help us basically do a lot of standard NLP tasks. And so I'm going to talk about what they released and we'll also have a look at it in code. But before we get to that, I want to talk about an interesting trend that I've been seeing of how people are doing sort of standard NLP. So there are a whole bunch of standard NLP tasks that are not generative AI or not even generative uses of LLMs etc. I'm talking about simple things like text classification being able to do things like sentiment being able to say if a piece of text belongs to one class or one group as opposed to another. and also being able to do tasks like named entity extraction, disambiguation, a whole bunch of these sort of traditional natural language processing tasks. And for a long time, a lot of these tasks were done with BERT models. So the BERT model came along at the end of 2018. This paper is actually an updated version of it that came out later in 2019. But the BERT model basically if we go back to what many people would think of as ancient history when talking about language models and transformers, the BERT model is a very different kind of model than what we've got now. So most people talk about the transformer architecture and this is what was invented in 2017 and came out of Google. But obviously not long after that people were starting to look at, okay, could this architecture be split up? And the original transformer contained two main key parts. the encoder model, the decoder model. So the decoder models are what we know as the GPT models and a lot of the open language models. These have changed now because they've gone to mixture of expert models, but a lot of the fundamentals are still the same with these. The encoder part of the transformer got used for the BERT model. And the BERT model was very useful at doing a lot of these standard tasks. So just like the standard LLMs today, they do the pre-training a little bit different than just next token prediction like what we see for most of the LLMs nowadays. They're actually doing things like filling in the gap, predicting whether one sentence semantically followed another sentence, that kind of thing. But the key thing here was that these BER models were really good for fine-tuning for very specific NLP tasks. So if we go back even earlier than the BERT models, people used to make unique architectures for different kinds of NLP tasks. And around this time, people were starting to think of if we could just have sort of one model that did it all. And when it came out, the B model was actually that. And it allowed people to basically fine-tune models for different things like sentiment, for classification, and also even do things at a token level. Although when this model came out, the context window was only like 512 tokens. So it's pretty small, but that was way bigger than what had been around with things like LSTMs and stuff like that before. So for a long time, these were standard ways for doing traditional natural language processing tasks. Now people ended up using different versions of this kind of thing where they would use like a distilled B. So it was very small. And interestingly, at the end of last year, we saw the introduction of the modern BERT. And at the time, I thought this was really interesting. These are pretty small models compared to today's LLMs. And in fact, even things like the original BER base model was only about 110 million parameters. These are tiny models. The distilled ones were, you know, way smaller than that. And for a long time, people were using these models in production to do various things like extracting names, extracting entities, doing things like sentiment analysis, doing things like text classification and they worked very well. And when this modern BE came out, I tried it out and I thought, okay, this is really good. This is going to be a big thing. But then over the last say six months, I've started to see a whole new pattern emerge that a lot of big companies are just not using these models anymore because they're finding that they can just get the same results by wrapping the text in a prompt and then giving it to a GPT 40 Mini or a Gemini Flash or a model, something like that. And in many ways, while that is overkill for what people actually need, it turns out that it's actually starting to become cheaper, that the fact of running these models and more importantly actually having things like having to have people on pager duty so if one of these models goes down that they've got people that can automatically get in there and fix the system. Turns out this being much more efficient to actually use APIs and LLM as a service for doing these natural language processing tasks. And that's what brings us to Lang Extract. So Lang Extract is a library made by Google specifically to do these kinds of tasks with Gemini. So the idea is that if you've got a very large amount of text and you want to be able to go through and extract these things out, you want to be able to give it some examples of what it is that you want. You want it to be able to go in there and get the entities, etc. And in many ways, even though these models have gotten better, you want to make some sort of checks to actually make sure that these things actually are in the text that you're passing in and not just hallucinations or errors from the model itself. So what lang extract is actually built for is these tasks of doing information extraction from text that you've already got. So you can see here that the first one is basically precise source grounding. So this is things like extracting the entities etc. And the idea here is that it's not only going to give you those back, it's going to give you where they actually are in the text and allow you to run checks yourself to see that they're actually there. And it is very funny that the way that they're showing the text here is actually very reminiscent or similar to a tool that's been around for a long time that people use for labeling data for training up your own small models to do a lot of these kind of tasks. So just a quick shout out this is Prodigy from a company called Explosion AI who makes spacey. still one of the best libraries in production for doing these kind of tasks with very small models and allowing you to basically customize and build your own pipelines etc. But you can see that as people are starting to do this more and more with these big models with things like Gemini flashlight becoming so cheap etc. customers want a way to be able to do it easy and to be able to put it all together and that's basically what Lang Extract does. So it allows us to do the source grounding. It allows us to get reliable structured outputs. You can actually give it some examples as few short examples and then give it your text which we'll look at the code in a minute. And it's even been set up for doing sort of long context information extraction. So if you've got something where you've got 100,000 words and you want to put it through this, this allows you to do that quite quickly. They also have what I showed you before the way to visualize these things if you want to have some way of visualizing it. And it does seem that they've made it so that it's not just going to be purely for the Gemini family of models, but you can also use it for open- source models. My guess is some people may end up using things like Gemma, like a lot of the cool Chinese models that we've seen come out recently. So, let's jump into just how this works. So, they've got an example in the code here. It's pretty simple. So, you can see that basically we're going to be importing this lang extract and then we can give it some few shot examples. In this case, we can give it some text. We can tell it what we would have expected it to find in that text. And then we can pass in some new text, get it to give us the output. We can see that they're using Gemini 2.5 Pro. I suspect that's probably overkill for this. You could probably get away with flash, probably even flashlight for a lot of the tasks. You can try them yourself, of course, and see what's going to work for you. And we can see that they've also got some nice things in here of where you can actually save this out and then basically get the visualization. So it will just make an HTML file where you can look at the visualization and stuff. So the cool thing with these in comparison to the BERT models is that the BERT models you'd have to collect a bunch of data. You'd have to train up a model and that's where things like Prodigy and Spacey come in is that they can help you do a lot of that stuff. With this we can define things quite quickly of here we can see that we've got condition dosage frequency medication. It can work out what the medication names are the dosage the frequency and we can get that back in a JSON format. So it's really easy. So there've been a bunch of libraries out there in the past that have used things like pyantic models to be able to get structured data out of LLMs that can do a lot of these things. And in some ways, you could think of this as being just a more polished version of something like that. So, let's jump into the code, have a play with it, see what we can do with this, and try it out. So, I've just set up a simple collab here. You can run this in Collab or you can run it locally on your machine, etc. It's quite easy to run. Basically, you just need to pip install lang extract. And then if you're going to be using Gemini, which is what I'm going to be showing here, you want to get a key for something like AI Studio. And then we just pass that in as the lang extract API key in here. So starting off with their example, you can see that you've got a prompt that basically defines what it is that you want to extract. And this can be N. It can be even more sort of vague things as well as I'll show you in the next example. You can see here they're going for characters, emotions, relationships in order of appearance. You can basically extract each of these. Don't paraphrase or overlap the entities. Provide meaningful attributes for each entity. Now, my guess is that for lots of different examples, you could play around with this prompt quite a bit to get something that's going to be really useful out. Then what you want to do is set up the sort of fshot learning examples. So here you just pass in some example text. So this is passing in some of the Romeo and Juliet text. It's showing that okay the extractions we want from here are going to be character emotion and relationship. So character an example of that would be Romeo and then we can state the emotional state you know that here he's feeling wonder right in this line that he's saying okay the emotion is coming from this but soft I guess I'm not sure I'd totally agree with that but anyway and then the feeling is gentle awe right which yeah I guess that describes that quite nicely and then a relationship here at this point the relationship Julia is the sun, right? This is what is the actual text. And then the type of relationship there is a metaphor. So this gives us our fshot examples. We then basically pass in the text that we actually really want to pass. In this case, it's just quite small amount of text. And if cheating a little bit in that we've already defined Romeo and Juliet in our example data, but of course like if you could do that makes a lot of sense. I'm going to show you where you don't have to do that. So let's see. Then the final bit is just to run it. So when we run it, we just basically do lx.extract. We pass in the input text, the one that we want to get the extractions from. We pass in our prompt. We pass in our fusot learning and then pass in the model that we're going to use. So they've used the 2.5 pro model. And you can see sure enough they get back things like character extraction Lady Juliet in here. And it's got then the actual place of where that is. It's got an emotional state. It's got all of these and they're not sorted in here. Anyway, one of the things that you'll want to do is sort them or get them in more useful sort of order perhaps. All right. So what I've done is I've taken an article which is this article that's just on Techrunch and it's quite a long article as well talking about a whole bunch of things about OpenAI and new models and it mentions a lot of different people. It mentions different models. It mentions different products and companies as well. So I basically just extracted the text out from that. We could have just used a scraper to get that. And but what I'm going to show you here is doing the same thing as their first one except we're going to play around with it a bit. So here we want to extract people's names, AI models, products, and company names in the order of appearance. Use the exact text for extractions. Provide meaningful related entities for each entity. For example, if it says Sam Alman, we would like it to be able to associate Sam Alolman with OpenAI and perhaps even OpenAI with Sam Alolman or with one of their products, that kind of thing. Okay. So, the example text that I'm giving is I'm deliberately giving it something in the same sort of world, but not what is actually in that article. All right. So, then when we look at the extractions, we can see that the first one is going to be person name. So in this case that's David Ha and then the company related to him is Sakana AI and then the company name is Sakana AI the relation employee David Ha and then the company name kind of AI the attribute here is employee I've just fixed the spelling on that to David Ha and then we've got AI model WM1 again company Sakana AI and then product AI scientist company Sakana AI I now I could have played around with these a little bit more and maybe even added some in there if I wanted to. Right? I'm not limited to just standard N in here. As you can see from AI model and product. All right. The text I'm going to put in is that Techrunch article and we're just going to pass that through. I'm just going to use Gemini 2.5 Flash in here. Okay. I'm going to run that through. You can see the time that it took. We can see the number of chunks etc. How many characters we processed and we can look at the output here. So we can see first off it was the person name Hunter Lightman and it's pretty hard to read it like that. So what we can actually do is just come down here and print these out. So if we want to basically just get the people's names you can see okay here I've basically printing out the person's name. So that's the class the extraction class. Now, I've got the extraction text being Hunter Lightman, right? That's what we actually care about. And then we've got the attributes. So, company that he's related to is OpenAI. And then we've got the characters where we could actually look that up. So, if we wanted to check this out, we could go through each of these. Okay. Second one up. Sam Alman, OpenAI. That makes sense. Mark Zuckerberg. Meta Shang Tia Xiao. Meta Super Intelligence. So, it's interesting that it's classified meta and meta and super intelligence as two separate things, which I think makes sense in this way. We've then got another name. We've then got a repeat like of the first one, but where they're just using his surname. And then we've got some other people like Ilas Suska. Okay, it's attributed to him to OpenAI. My guess is that's because of his time at OpenAI. That is what the article is referring to. Mark Chen's head of research at OpenAI, so that makes sense. All right, we've got a good set of names that are coming out there. And we can see that we've got doubles in them as well. So, what you can do is something like this. So, for the company's mention, first just going to go through and just get the unique companies cuz if we wanted to use just the unique ones, we can do that. I'm also going to print them out so we can see them as they go along. Sure enough, we've got lots of mentions of OpenAI, lots of mentions of some other companies, and then a number of companies just mentioned once in there. But we can see Google, OpenAI, philanthropic, all meta, all mentioned many times. But if we use that unique, we can basically just get a list of all the companies that are actually mentioned in there. So, we can see here it probably makes sense that it associated Ilia Satska with OpenI because I'm not seeing his company mentioned in here. Now finally just to finish up we can look at the AI models and the products. We can see that okay shonoff forgets that chat GBT is a product here but it then also refers to chat GPT as an AI model. It's interesting because that's what a lot of humans do. So it' be interesting to look at that. We can see okay AI model01 Alph Go GBT series Qstar Strawberry all of these things make sense for AI models product01 though so my guess is that it's taking it from the context of when do people refer to it as a product versus as an AI model and this is a good example of where I've probably been a little bit vague in having both of these I perhaps should have given some more direction in the prompt If I wanted to do that kind of thing, we can see that it's done a good job for some of them. Like Claude Code is a product, cursor is a product, codeex, I'd say probably is more a product than a model. Comet is a product. So I'd say it's done well with most of the actual classifications for product and AI model, but that's certainly something that we could improve on by reworking the prompt and playing around with this. The other thing too is at this stage I would then go through it and test it with flashlight and see okay does that work and then I would test it with maybe flash 2.0 Oh, and see, okay, with older versions of the model that are cheaper, is that a good tradeoff? But you can see from setting this up very quickly, we could have something that went across everything that we scraped, extracted all the NUR and stored that as metadata in a rag system or as metadata on any sort of analysis of this that we wanted to do. So this becomes really useful if you're doing anything where you're passing over news. If you're trying to extract out financial information related to companies or something like that, don't could even put information about some of those things in the actual prompt. So this is what allows you to do it on the fly. And if you really did still want to train one of the other models, you can use something like this to make your training data so that you can then use maybe 2.5 Pro, pay the expensive cost here, and then use this as training data to make a much smaller, faster model if you wanted to do that. Anyway, overall, I think Lang Extraction is a nice little library that they've released. It's certainly something people can use for real world examples and put it into production very easily. All right, as always, if you've got any questions, please put them in the comments below. If you found the video useful, please click like and subscribe, and I will talk to you in the next video. Bye for now.